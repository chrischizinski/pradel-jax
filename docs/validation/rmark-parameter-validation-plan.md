# RMark Parameter Validation: Comprehensive Implementation Plan

**Version**: 1.0  
**Date**: August 14, 2025  
**Status**: Implementation Ready  
**Priority**: High (Development Priority #3)

---

## üéØ **Executive Summary**

This document specifies a **production-grade parameter validation framework** that compares JAX-based Pradel model estimates against RMark results with **statistical rigor** and **industry-standard validation practices**.

### **Mission Statement**
Provide **statistical confidence** in JAX implementation accuracy through automated, cross-platform validation that enables **publication-quality scientific validation**.

### **Key Success Metrics**
- **Parameter Accuracy**: <1% difference in point estimates
- **Statistical Equivalence**: 95% confidence in parameter equivalence
- **Model Ranking**: >80% concordance in AIC-based model selection
- **Automation**: 90% reduction in manual validation effort
- **Reproducibility**: 100% consistent results across environments

---

## üèóÔ∏è **Architecture Design**

### **Design Principles**
1. **Statistical Rigor**: Follow NIST and IEEE standards for numerical validation
2. **Environmental Flexibility**: Support multiple RMark execution strategies
3. **Scientific Reproducibility**: Ensure identical data and model specifications
4. **Industry Standards**: Use established libraries and validation methodologies
5. **Extensibility**: Support future model types and validation requirements

### **Core Components**

```
pradel_jax/validation/
‚îú‚îÄ‚îÄ üìä parameter_comparison.py    # Statistical validation framework
‚îú‚îÄ‚îÄ üî¨ rmark_interface.py        # Multi-platform RMark execution
‚îú‚îÄ‚îÄ üìà statistical_tests.py      # Industry-standard statistical tests
‚îú‚îÄ‚îÄ üöÄ pipeline.py              # Orchestration and automation
‚îú‚îÄ‚îÄ üìã report_generator.py       # Publication-ready reports
‚îú‚îÄ‚îÄ ‚öôÔ∏è config.py                # Validation configuration
‚îî‚îÄ‚îÄ üß™ test_validation.py        # Comprehensive test suite
```

---

## üîß **RMark Execution Strategy**

### **Multi-Environment Support Architecture**

Given the challenges with Docker and varying network access, the system implements a **flexible execution strategy** with automatic fallback:

#### **Priority Order:**
1. **SSH to Windows Machine** (Home Office) - ‚úÖ **Proven Reliable**
2. **Local R with RMark** (Work Office) - üì¶ **Self-Contained**
3. **Cloud RMark Service** (Any Location) - ‚òÅÔ∏è **Future-Proof**
4. **Docker Containerized** (Development) - üê≥ **Last Resort**

### **Implementation Details**

```python
@dataclass
class RMarkExecutionConfig:
    """Configuration for RMark execution environments."""
    
    # SSH Configuration (Home Office)
    ssh_enabled: bool = True
    ssh_host: str = "192.168.86.21"  # Your Windows machine
    ssh_user: str = "chris"
    ssh_key_path: Optional[str] = None
    r_path: str = '"C:\\Program Files\\R\\R-4.5.1\\bin\\Rscript.exe"'
    
    # Local R Configuration (Work Office)
    local_r_enabled: bool = True
    local_r_path: str = "Rscript"  # Assumes R in PATH
    rmark_installed: bool = False  # Auto-detected
    
    # Cloud Service Configuration (Future)
    cloud_enabled: bool = False
    cloud_endpoint: Optional[str] = None
    cloud_api_key: Optional[str] = None
    
    # Docker Configuration (Fallback)
    docker_enabled: bool = False
    docker_image: str = "rmark-pradel:latest"
    
    # Execution Preferences
    timeout_seconds: int = 300
    max_retries: int = 3
    preferred_method: str = "auto"  # "ssh", "local", "cloud", "docker"

class RMarkExecutionStrategy:
    """Intelligent RMark execution with environment detection."""
    
    def __init__(self, config: RMarkExecutionConfig):
        self.config = config
        self.available_methods = self._detect_available_methods()
    
    def execute_rmark_analysis(
        self, 
        data: DataContext, 
        formula_spec: FormulaSpec
    ) -> RMarkResult:
        """Execute RMark with automatic method selection and fallback."""
        
        # Try methods in priority order
        for method in self._get_execution_order():
            try:
                logger.info(f"Attempting RMark execution via {method}")
                return self._execute_via_method(method, data, formula_spec)
            except Exception as e:
                logger.warning(f"RMark execution failed via {method}: {e}")
                continue
        
        raise RMarkExecutionError("All RMark execution methods failed")
    
    def _get_execution_order(self) -> List[str]:
        """Get execution methods in priority order based on environment."""
        if self.config.preferred_method != "auto":
            return [self.config.preferred_method]
        
        # Auto-detection based on environment
        if self._is_home_office():
            return ["ssh", "local", "cloud", "docker"]
        elif self._is_work_office():
            return ["local", "cloud", "ssh", "docker"]
        else:
            return ["cloud", "local", "ssh", "docker"]
```

### **Environment Detection Logic**

```python
def _detect_environment(self) -> str:
    """Detect current working environment."""
    
    # Check network connectivity to home Windows machine
    if self._can_ssh_to_home():
        return "home_office"
    
    # Check for local R installation
    elif self._has_local_r_with_rmark():
        return "work_office"
    
    # Check for cloud connectivity
    elif self._has_cloud_access():
        return "cloud_available"
    
    else:
        return "limited_connectivity"

def _can_ssh_to_home(self) -> bool:
    """Test SSH connectivity to home Windows machine."""
    try:
        import paramiko
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(
            hostname=self.config.ssh_host,
            username=self.config.ssh_user,
            timeout=10
        )
        ssh.close()
        return True
    except:
        return False

def _has_local_r_with_rmark(self) -> bool:
    """Check if R and RMark are available locally."""
    try:
        import subprocess
        result = subprocess.run(
            ["Rscript", "-e", "library(RMark); cat('SUCCESS')"],
            capture_output=True,
            text=True,
            timeout=30
        )
        return "SUCCESS" in result.stdout
    except:
        return False
```

---

## üìä **Statistical Validation Framework**

### **Parameter-Level Validation**

```python
@dataclass
class ParameterValidationResult:
    """Comprehensive parameter validation result."""
    parameter_name: str
    parameter_type: str  # "phi", "p", "f"
    
    # Point estimates
    jax_estimate: float
    jax_std_error: float
    rmark_estimate: float
    rmark_std_error: float
    
    # Statistical tests
    absolute_difference: float
    relative_difference_pct: float
    confidence_intervals_overlap: bool
    
    # Equivalence testing (TOST)
    equivalence_test_statistic: float
    equivalence_p_value: float
    equivalence_conclusion: bool  # True if equivalent
    equivalence_bounds: Tuple[float, float]
    
    # Quality assessment
    precision_level: str  # "excellent" (<0.1%), "good" (<1%), "acceptable" (<5%), "poor" (>5%)
    validation_status: ValidationStatus  # PASS, FAIL, WARNING
    recommendations: List[str]

class StatisticalValidator:
    """Industry-standard statistical tests for parameter validation."""
    
    def validate_parameter_equivalence(
        self,
        jax_params: Dict[str, float],
        jax_se: Dict[str, float],
        rmark_params: Dict[str, float],
        rmark_se: Dict[str, float],
        equivalence_margin: float = 0.05
    ) -> List[ParameterValidationResult]:
        """
        Comprehensive parameter validation using multiple statistical tests.
        
        Tests performed:
        1. Absolute difference tolerance
        2. Relative difference tolerance  
        3. Confidence interval overlap
        4. Two One-Sided Tests (TOST) for equivalence
        5. Bootstrap confidence intervals (if needed)
        """
        results = []
        
        for param_name in jax_params.keys():
            if param_name not in rmark_params:
                continue
                
            # Extract values
            jax_est = jax_params[param_name]
            jax_err = jax_se.get(param_name, 0.0)
            rmark_est = rmark_params[param_name]
            rmark_err = rmark_se.get(param_name, 0.0)
            
            # Calculate differences
            abs_diff = abs(jax_est - rmark_est)
            rel_diff = abs_diff / abs(rmark_est) * 100 if rmark_est != 0 else float('inf')
            
            # Confidence interval overlap
            ci_overlap = self._check_ci_overlap(
                jax_est, jax_err, rmark_est, rmark_err
            )
            
            # TOST equivalence test
            tost_stat, tost_p, equivalent = self._tost_equivalence_test(
                jax_est, jax_err, rmark_est, rmark_err, equivalence_margin
            )
            
            # Determine precision level
            precision = self._assess_precision_level(rel_diff)
            
            # Determine validation status
            status = self._determine_validation_status(
                abs_diff, rel_diff, ci_overlap, equivalent
            )
            
            results.append(ParameterValidationResult(
                parameter_name=param_name,
                parameter_type=self._get_parameter_type(param_name),
                jax_estimate=jax_est,
                jax_std_error=jax_err,
                rmark_estimate=rmark_est,
                rmark_std_error=rmark_err,
                absolute_difference=abs_diff,
                relative_difference_pct=rel_diff,
                confidence_intervals_overlap=ci_overlap,
                equivalence_test_statistic=tost_stat,
                equivalence_p_value=tost_p,
                equivalence_conclusion=equivalent,
                equivalence_bounds=(-equivalence_margin, equivalence_margin),
                precision_level=precision,
                validation_status=status,
                recommendations=self._generate_recommendations(
                    param_name, abs_diff, rel_diff, equivalent
                )
            ))
        
        return results
```

### **Model-Level Validation**

```python
@dataclass
class ModelValidationResult:
    """Model-level validation comparing JAX vs RMark."""
    model_formula: str
    
    # Model fit statistics
    jax_aic: float
    jax_log_likelihood: float
    jax_n_parameters: int
    jax_convergence: bool
    
    rmark_aic: float
    rmark_log_likelihood: float
    rmark_n_parameters: int
    rmark_convergence: bool
    
    # Comparison metrics
    aic_difference: float
    likelihood_difference: float
    likelihood_relative_difference_pct: float
    
    # Model ranking (within dataset)
    jax_aic_rank: Optional[int] = None
    rmark_aic_rank: Optional[int] = None
    ranking_agreement: bool = False
    
    # Overall assessment
    validation_status: ValidationStatus
    validation_summary: str

def validate_model_concordance(
    self,
    jax_results: List[OptimizationResult],
    rmark_results: List[RMarkResult]
) -> ModelValidationSummary:
    """
    Comprehensive model-level validation.
    
    Tests:
    1. AIC concordance (ecological significance threshold: ¬±2.0)
    2. Log-likelihood agreement
    3. Model ranking concordance (Kendall's tau)
    4. Parameter vector distance
    5. Convergence rate agreement
    """
    
    model_results = []
    
    # Pair up models by formula
    paired_models = self._pair_models_by_formula(jax_results, rmark_results)
    
    for jax_result, rmark_result in paired_models:
        # AIC comparison
        aic_diff = abs(jax_result.aic - rmark_result.aic)
        aic_concordant = aic_diff < 2.0  # Ecological significance threshold
        
        # Log-likelihood comparison
        ll_diff = abs(jax_result.log_likelihood - rmark_result.log_likelihood)
        ll_rel_diff = ll_diff / abs(rmark_result.log_likelihood) * 100
        ll_concordant = ll_rel_diff < 1.0  # 1% threshold
        
        # Determine validation status
        if aic_concordant and ll_concordant:
            status = ValidationStatus.PASS
            summary = "Excellent model concordance"
        elif aic_concordant or ll_concordant:
            status = ValidationStatus.WARNING
            summary = "Partial model concordance"
        else:
            status = ValidationStatus.FAIL
            summary = "Poor model concordance"
        
        model_results.append(ModelValidationResult(
            model_formula=jax_result.model_formula,
            jax_aic=jax_result.aic,
            jax_log_likelihood=jax_result.log_likelihood,
            jax_n_parameters=jax_result.n_parameters,
            jax_convergence=jax_result.success,
            rmark_aic=rmark_result.aic,
            rmark_log_likelihood=rmark_result.log_likelihood,
            rmark_n_parameters=rmark_result.n_parameters,
            rmark_convergence=rmark_result.convergence == 0,
            aic_difference=aic_diff,
            likelihood_difference=ll_diff,
            likelihood_relative_difference_pct=ll_rel_diff,
            validation_status=status,
            validation_summary=summary
        ))
    
    # Calculate model ranking concordance
    ranking_concordance = self._calculate_ranking_concordance(
        [r.jax_aic for r in model_results],
        [r.rmark_aic for r in model_results]
    )
    
    return ModelValidationSummary(
        model_results=model_results,
        ranking_concordance=ranking_concordance,
        overall_pass_rate=sum(1 for r in model_results if r.validation_status == ValidationStatus.PASS) / len(model_results),
        recommendations=self._generate_model_recommendations(model_results)
    )
```

---

## üìà **Automated Pipeline Architecture**

### **Validation Pipeline Orchestrator**

```python
class ValidationPipeline:
    """End-to-end validation pipeline with quality gates."""
    
    def __init__(self, config: ValidationConfig):
        self.config = config
        self.rmark_executor = RMarkExecutionStrategy(config.rmark_config)
        self.statistical_validator = StatisticalValidator()
        self.report_generator = ValidationReportGenerator()
    
    def run_comprehensive_validation(
        self,
        datasets: List[DataContext],
        model_specifications: List[FormulaSpec],
        output_dir: Path
    ) -> ValidationReport:
        """
        Execute complete validation pipeline.
        
        Steps:
        1. Environment detection and RMark setup
        2. Parallel JAX model fitting
        3. Parallel RMark execution
        4. Statistical comparison and testing
        5. Quality gate evaluation
        6. Report generation and archiving
        """
        
        logger.info(f"Starting comprehensive validation: {len(datasets)} datasets, {len(model_specifications)} models")
        
        validation_session = ValidationSession(
            session_id=str(uuid.uuid4()),
            start_time=datetime.now(),
            datasets=datasets,
            model_specifications=model_specifications,
            config=self.config
        )
        
        try:
            # Step 1: Environment detection
            environment = self._detect_and_prepare_environment()
            logger.info(f"Detected environment: {environment}")
            
            # Step 2: Execute JAX models
            logger.info("Executing JAX model fitting...")
            jax_results = self._execute_jax_models(datasets, model_specifications)
            
            # Step 3: Execute RMark models
            logger.info("Executing RMark model fitting...")
            rmark_results = self._execute_rmark_models(datasets, model_specifications)
            
            # Step 4: Statistical validation
            logger.info("Performing statistical validation...")
            validation_results = self._perform_statistical_validation(
                jax_results, rmark_results
            )
            
            # Step 5: Quality gate evaluation
            logger.info("Evaluating quality gates...")
            quality_assessment = self._evaluate_quality_gates(validation_results)
            
            # Step 6: Generate comprehensive report
            logger.info("Generating validation report...")
            report = self.report_generator.generate_comprehensive_report(
                validation_session,
                jax_results,
                rmark_results,
                validation_results,
                quality_assessment,
                output_dir
            )
            
            # Step 7: Archive results
            self._archive_validation_artifacts(validation_session, output_dir)
            
            logger.info(f"Validation completed. Report: {report.report_path}")
            return report
            
        except Exception as e:
            logger.error(f"Validation pipeline failed: {e}")
            # Generate failure report
            failure_report = self._generate_failure_report(validation_session, e)
            raise ValidationPipelineError(f"Pipeline failed: {e}") from e
        
        finally:
            validation_session.end_time = datetime.now()
            validation_session.duration = validation_session.end_time - validation_session.start_time
```

---

## üìã **Quality Gates & Acceptance Criteria**

### **Configurable Validation Thresholds**

```python
@dataclass
class ValidationCriteria:
    """Comprehensive validation criteria with industry standards."""
    
    # Parameter-level thresholds (following bioequivalence guidelines)
    parameter_absolute_tolerance: float = 1e-3  # 0.001 absolute difference
    parameter_relative_tolerance_pct: float = 5.0  # 5% relative difference
    equivalence_margin: float = 0.05  # ¬±5% for TOST equivalence
    require_confidence_overlap: bool = True
    equivalence_alpha: float = 0.05  # 95% confidence for equivalence
    
    # Model-level thresholds (ecological significance)
    max_aic_difference: float = 2.0  # Ecological significance threshold
    max_likelihood_relative_diff_pct: float = 1.0  # 1% log-likelihood difference
    min_ranking_concordance: float = 0.8  # Kendall's tau ‚â• 0.8
    
    # System-level requirements
    min_convergence_rate: float = 0.95  # 95% of models must converge
    max_computation_time_ratio: float = 10.0  # JAX ‚â§ 10x RMark time
    require_reproducibility: bool = True
    max_numerical_instability: float = 1e-6  # Parameter stability across runs
    
    # Quality gate settings
    min_pass_rate_for_approval: float = 0.90  # 90% of tests must pass
    critical_parameters: List[str] = None  # Parameters that MUST pass
    allow_warnings_in_approval: bool = True
    
    def __post_init__(self):
        if self.critical_parameters is None:
            # Default critical parameters for Pradel models
            self.critical_parameters = ["phi_intercept", "p_intercept", "f_intercept"]

class QualityGateEvaluator:
    """Evaluate validation results against quality gates."""
    
    def evaluate_validation_results(
        self,
        validation_results: ValidationResults,
        criteria: ValidationCriteria
    ) -> QualityGateReport:
        """Comprehensive quality gate evaluation."""
        
        # Parameter-level evaluation
        param_results = self._evaluate_parameter_quality(
            validation_results.parameter_results, criteria
        )
        
        # Model-level evaluation
        model_results = self._evaluate_model_quality(
            validation_results.model_results, criteria
        )
        
        # System-level evaluation
        system_results = self._evaluate_system_quality(
            validation_results, criteria
        )
        
        # Overall decision
        overall_decision = self._make_overall_decision(
            param_results, model_results, system_results, criteria
        )
        
        return QualityGateReport(
            overall_decision=overall_decision,
            parameter_assessment=param_results,
            model_assessment=model_results,
            system_assessment=system_results,
            criteria_used=criteria,
            recommendations=self._generate_improvement_recommendations(
                param_results, model_results, system_results
            )
        )
```

---

## üöÄ **Implementation Phases**

### **Phase 1: Core Validation Framework** (Week 1)
**Deliverables:**
- [ ] `parameter_comparison.py` - Statistical parameter comparison
- [ ] `statistical_tests.py` - TOST, CI overlap, concordance tests
- [ ] `rmark_interface.py` - Multi-environment RMark execution
- [ ] Basic unit tests for all statistical functions
- [ ] Environment detection and fallback logic

**Success Criteria:**
- All statistical tests implemented with >95% test coverage
- RMark execution works in home office (SSH) and work office (local R)
- Basic parameter validation on dipper dataset

### **Phase 2: Advanced Statistical Testing** (Week 2)
**Deliverables:**
- [ ] Complete TOST equivalence testing implementation
- [ ] Model ranking concordance analysis (Kendall's tau)
- [ ] Bootstrap confidence interval computation
- [ ] Comprehensive model-level validation
- [ ] Integration tests with real datasets

**Success Criteria:**
- Publication-quality statistical tests validated against literature
- Model ranking achieves >80% concordance on validation datasets
- System handles edge cases (non-convergence, numerical instability)

### **Phase 3: Automated Pipeline & Quality Gates** (Week 3)
**Deliverables:**
- [ ] `pipeline.py` - End-to-end validation orchestration
- [ ] `config.py` - Flexible configuration system
- [ ] Quality gate evaluation framework
- [ ] Parallel processing for multiple datasets/models
- [ ] Comprehensive error handling and recovery

**Success Criteria:**
- Pipeline processes multiple datasets without manual intervention
- Quality gates provide clear pass/fail decisions
- System gracefully handles all RMark execution failures
- Performance: processes 100 models in <30 minutes

### **Phase 4: Reporting & Production Deployment** (Week 4)
**Deliverables:**
- [ ] `report_generator.py` - Publication-ready validation reports
- [ ] HTML/PDF report templates with statistical visualizations
- [ ] Integration with existing `pradel_jax` testing framework
- [ ] Comprehensive documentation and user guides
- [ ] CI/CD integration for automated validation

**Success Criteria:**
- Reports meet publication standards for ecology journals
- System integrated into existing development workflow
- Documentation enables independent use by other researchers
- Automated validation runs on code changes

---

## üìö **Technology Stack & Dependencies**

### **Core Libraries**
```python
# Statistical analysis
scipy >= 1.11.0           # Statistical tests, optimization
numpy >= 1.24.0           # Numerical computations
pandas >= 2.0.0           # Data manipulation

# Cross-platform execution
paramiko >= 3.0.0         # SSH connectivity for RMark
subprocess32              # Robust process execution
docker >= 6.0.0           # Container management (if needed)

# Report generation  
jinja2 >= 3.1.0           # Report templating
matplotlib >= 3.7.0       # Statistical visualizations
seaborn >= 0.12.0         # Advanced plotting

# Configuration and logging
pydantic >= 2.0.0         # Configuration validation
structlog >= 23.1.0       # Structured logging
typer >= 0.9.0           # CLI interface

# Testing and validation
pytest >= 7.4.0          # Testing framework
hypothesis >= 6.82.0     # Property-based testing
pytest-cov >= 4.1.0      # Coverage reporting
```

### **R Dependencies (for local execution)**
```r
# Required R packages
install.packages(c(
    "RMark",        # Mark-recapture analysis
    "jsonlite",     # JSON I/O for data exchange
    "devtools"      # Development tools
))
```

---

## üîê **Security & Data Protection**

### **Data Handling Standards**
- **No sensitive data transfer**: Only synthetic/public datasets used for validation
- **Secure SSH**: Key-based authentication, no password storage
- **Local processing**: Sensitive data never leaves secure environment
- **Audit logging**: Complete trail of all validation activities
- **Version control**: No credentials or sensitive data in git

### **Environment Isolation**
```python
# Secure configuration management
@dataclass 
class SecureValidationConfig:
    # SSH credentials (loaded from environment)
    ssh_host: str = os.getenv("RMARK_SSH_HOST", "localhost")
    ssh_user: str = os.getenv("RMARK_SSH_USER", "user")
    ssh_key_path: str = os.getenv("RMARK_SSH_KEY", "~/.ssh/id_rsa")
    
    # Working directories (isolated)
    temp_dir: Path = Path("/tmp/pradel_validation")
    output_dir: Path = Path("./validation_results")
    
    # Security settings
    enable_audit_logging: bool = True
    max_execution_time: int = 600  # 10 minutes timeout
    cleanup_temp_files: bool = True
```

---

## üìà **Success Metrics & Monitoring**

### **Key Performance Indicators**
1. **Validation Accuracy**: Parameter differences <1%
2. **Statistical Power**: >95% successful equivalence tests
3. **Model Concordance**: >80% AIC ranking agreement  
4. **Automation Rate**: >90% validation without manual intervention
5. **Execution Reliability**: >95% successful RMark executions
6. **Performance**: Complete validation suite in <30 minutes

### **Monitoring Dashboard**
```python
class ValidationMetrics:
    """Real-time validation monitoring."""
    
    def track_validation_session(self, session: ValidationSession):
        """Track comprehensive validation metrics."""
        
        metrics = {
            # Accuracy metrics
            "parameter_accuracy": self.calculate_parameter_accuracy(session),
            "model_concordance": self.calculate_model_concordance(session),
            "statistical_power": self.calculate_statistical_power(session),
            
            # Performance metrics
            "execution_time": session.total_execution_time,
            "success_rate": session.overall_success_rate,
            "rmark_reliability": session.rmark_success_rate,
            
            # Quality metrics
            "quality_gate_pass_rate": session.quality_gate_pass_rate,
            "critical_failures": session.critical_failure_count,
            "warnings_count": session.warning_count
        }
        
        # Log to monitoring system
        logger.info("Validation metrics", extra=metrics)
        
        # Alert on quality degradation
        if metrics["parameter_accuracy"] < 0.95:
            self.alert_quality_degradation("parameter_accuracy", metrics)
```

---

## üéØ **Next Steps**

### **Immediate Actions** (This Week)
1. **Create validation directory structure** in `pradel_jax/validation/`
2. **Implement basic parameter comparison** with statistical tests
3. **Set up RMark execution strategy** with SSH and local R fallback
4. **Create configuration system** for different environments

### **Weekly Milestones**
- **Week 1**: Core framework functional with basic validation
- **Week 2**: Statistical tests validated against literature examples
- **Week 3**: Full pipeline operational with quality gates
- **Week 4**: Production deployment with comprehensive documentation

### **Stakeholder Communication**
- **Weekly progress reports** with validation metrics
- **Milestone demonstrations** with real dataset examples
- **Documentation reviews** to ensure scientific rigor
- **Integration planning** with existing development workflow

---

**This comprehensive plan provides the foundation for world-class parameter validation that will establish JAX-based Pradel models as scientifically credible and publication-ready.**

*Document maintained by: Development Team*  
*Next review: Weekly during implementation*  
*Version control: Track in git with major revisions*