#!/usr/bin/env python3
"""
Production Nebraska Analysis with Optimized Performance and Checkpoint/Resume
- Vectorized likelihood computation for 100-1000x speedup
- Checkpoint/resume functionality for long-running jobs
- Progress tracking and time estimation
- Memory-efficient processing for large datasets
"""

import pandas as pd
import numpy as np
import pradel_jax as pj
from pradel_jax.models.pradel_optimized import OptimizedPradelModel
from pradel_jax.optimization import optimize_model
from pradel_jax.optimization.strategy import OptimizationStrategy
import sys
import time
import json
import pickle
from pathlib import Path
from typing import Optional, Dict, Any
import argparse

class AnalysisCheckpoint:
    """Manages checkpointing and resuming of long-running analysis."""
    
    def __init__(self, checkpoint_file: str):
        self.checkpoint_file = checkpoint_file
        self.checkpoint_dir = Path(checkpoint_file).parent
        self.checkpoint_dir.mkdir(exist_ok=True)
    
    def save_checkpoint(self, state: Dict[str, Any]):
        """Save analysis state to checkpoint file."""
        print(f"üíæ Saving checkpoint to {self.checkpoint_file}")
        with open(self.checkpoint_file, 'wb') as f:
            pickle.dump(state, f)
    
    def load_checkpoint(self) -> Optional[Dict[str, Any]]:
        """Load analysis state from checkpoint file."""
        if Path(self.checkpoint_file).exists():
            print(f"üìÇ Loading checkpoint from {self.checkpoint_file}")
            with open(self.checkpoint_file, 'rb') as f:
                return pickle.load(f)
        return None
    
    def delete_checkpoint(self):
        """Delete checkpoint file after successful completion."""
        if Path(self.checkpoint_file).exists():
            Path(self.checkpoint_file).unlink()
            print(f"üóëÔ∏è  Deleted checkpoint file")

def prepare_nebraska_data(data_file: str, sample_size: Optional[int] = None, seed: int = 42) -> pd.DataFrame:
    """Prepare Nebraska data with consistent seeding."""
    print(f"üìÇ Loading Nebraska data from: {data_file}")
    
    np.random.seed(seed)  # Ensure reproducible sampling
    
    # Load data
    if sample_size:
        total_rows = sum(1 for _ in open(data_file)) - 1
        skip_rows = sorted(np.random.choice(range(1, total_rows + 1), 
                                          size=total_rows - sample_size, 
                                          replace=False))
        data = pd.read_csv(data_file, skiprows=skip_rows)
        print(f"   Loaded {len(data):,} individuals (sampled from {total_rows:,})")
    else:
        data = pd.read_csv(data_file)
        print(f"   Loaded {len(data):,} individuals")
    
    # Convert to Pradel-JAX format
    print("üîß Converting to Pradel-JAX format...")
    
    years = list(range(2016, 2025))
    year_columns = [f'Y{year}' for year in years]
    age_columns = [f'age_{year}' for year in years]
    tier_columns = [f'tier_{year}' for year in years]
    
    encounter_data = (data[year_columns].values > 0).astype(int)
    
    formatted_data = []
    for i, (idx, row) in enumerate(data.iterrows()):
        individual_record = {
            'individual': row['person_id'],
            'gender': row['gender'] if pd.notna(row['gender']) else 0,
        }
        
        # Add encounter histories
        for j in range(len(year_columns)):
            individual_record[f'occasion_{j+1}'] = encounter_data[i, j]
        
        # Add time-varying covariates  
        for j, (age_col, tier_col) in enumerate(zip(age_columns, tier_columns)):
            individual_record[f'age_{j+1}'] = row[age_col] if pd.notna(row[age_col]) else 0
            individual_record[f'tier_{j+1}'] = row[tier_col] if pd.notna(row[tier_col]) else 0
            
        formatted_data.append(individual_record)
    
    result_df = pd.DataFrame(formatted_data)
    
    # Data quality summary
    occasions = [col for col in result_df.columns if col.startswith('occasion_')]
    total_detections = result_df[occasions].sum().sum()
    total_possible = len(result_df) * len(occasions)
    detection_rate = total_detections / total_possible
    
    print(f"   - Occasions: {len(occasions)}")
    print(f"   - Detection rate: {detection_rate:.3f}")
    print(f"   - Never detected: {(result_df[occasions].sum(axis=1) == 0).sum()}")
    
    return result_df

def fit_model_optimized(model, formula_spec, data_context, model_name: str) -> Dict[str, Any]:
    """Fit model using optimized Pradel implementation."""
    start_time = time.time()
    
    try:
        result = optimize_model(
            objective_function=lambda params: -model.log_likelihood(params, data_context, 
                                                                   model.build_design_matrices(formula_spec, data_context)),
            initial_parameters=model.get_initial_parameters(data_context, 
                                                           model.build_design_matrices(formula_spec, data_context)),
            context=data_context,
            bounds=model.get_parameter_bounds(data_context, 
                                            model.build_design_matrices(formula_spec, data_context)),
            preferred_strategy=OptimizationStrategy.HYBRID
        )
        
        fit_time = time.time() - start_time
        
        if result.success:
            opt_result = result.result
            ll = -opt_result.fun
            k = len(opt_result.x)
            aic = 2 * k - 2 * ll
            
            return {
                'success': True,
                'parameters': opt_result.x.tolist(),
                'log_likelihood': ll,
                'aic': aic,
                'n_evaluations': opt_result.nfev,
                'n_parameters': k,
                'strategy_used': result.strategy_used,
                'fit_time': fit_time,
                'formula_spec': {
                    'phi': formula_spec.phi,
                    'p': formula_spec.p, 
                    'f': formula_spec.f
                },
                'convergence_message': opt_result.message
            }
        else:
            return {
                'success': False,
                'error': 'Optimization failed',
                'fit_time': fit_time,
                'message': getattr(result.result, 'message', 'Unknown error') if hasattr(result, 'result') else 'Unknown error'
            }
            
    except Exception as e:
        fit_time = time.time() - start_time
        return {
            'success': False,
            'error': str(e),
            'fit_time': fit_time
        }

def export_results_with_progress(results: list, base_name: str, completed_models: int, total_models: int):
    """Export results with progress information."""
    
    # CSV export
    csv_file = f"{base_name}_progress.csv"
    csv_data = []
    
    for model_info in results:
        row = {
            'model': model_info['name'],
            'success': model_info['result']['success'],
            'completed': True
        }
        
        if model_info['result']['success']:
            result = model_info['result']
            row.update({
                'aic': result['aic'],
                'log_likelihood': result['log_likelihood'],
                'n_parameters': result['n_parameters'],
                'strategy_used': result.get('strategy_used', 'unknown'),
                'fit_time': result.get('fit_time', 0),
                'n_evaluations': result.get('n_evaluations', 0)
            })
            
            for i, param in enumerate(result['parameters']):
                row[f'param_{i+1}'] = param
        else:
            row.update({
                'aic': np.nan,
                'log_likelihood': np.nan,
                'error': result.get('error', 'Unknown')
            })
        
        csv_data.append(row)
    
    df = pd.DataFrame(csv_data)
    df.to_csv(csv_file, index=False)
    
    print(f"üìä Progress: {completed_models}/{total_models} models completed")
    print(f"üíæ Results saved to {csv_file}")

def run_production_analysis(
    sample_size: int,
    output_dir: str = "results",
    checkpoint_interval: int = 5,
    resume: bool = False
):
    """Run production analysis with checkpointing."""
    
    # Setup
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    base_name = f"nebraska_production_{sample_size}ind_{timestamp}"
    checkpoint_file = output_path / f"{base_name}_checkpoint.pkl"
    
    checkpoint_manager = AnalysisCheckpoint(str(checkpoint_file))
    
    # Try to resume from checkpoint
    checkpoint_data = None
    if resume:
        checkpoint_data = checkpoint_manager.load_checkpoint()
    
    if checkpoint_data:
        print("üîÑ Resuming from checkpoint...")
        results = checkpoint_data['results']
        completed_models = checkpoint_data['completed_models']
        data_context = checkpoint_data['data_context']
        model_specifications = checkpoint_data['model_specifications']
        analysis_start_time = checkpoint_data['analysis_start_time']
        sample_size = checkpoint_data['sample_size']
    else:
        print("üöÄ Starting fresh analysis...")
        
        # Prepare data
        data_file = "data/encounter_histories_ne_clean.csv"
        nebraska_data = prepare_nebraska_data(data_file, sample_size)
        
        # Load into Pradel-JAX
        temp_file = f"temp_production_{sample_size}.csv"
        nebraska_data.to_csv(temp_file, index=False)
        data_context = pj.load_data(temp_file)
        
        # Clean up temp file
        Path(temp_file).unlink()
        
        print(f"‚úÖ Loaded: {data_context.n_individuals:,} individuals, {data_context.n_occasions} occasions")
        
        # Define model specifications
        phi_formulas = [
            "~1", "~1 + gender", "~1 + age_1", "~1 + tier_1",
            "~1 + gender + age_1", "~1 + gender + tier_1", 
            "~1 + age_1 + tier_1", "~1 + gender + age_1 + tier_1"
        ]
        
        f_formulas = [
            "~1", "~1 + gender", "~1 + age_1", "~1 + tier_1",
            "~1 + gender + age_1", "~1 + gender + tier_1", 
            "~1 + age_1 + tier_1", "~1 + gender + age_1 + tier_1"
        ]
        
        def create_model_name(formula):
            if formula == "~1":
                return "1"
            return formula.replace("~1 + ", "").replace("age_1", "age").replace("tier_1", "tier")
        
        model_specifications = [
            {
                'name': f"œÜ({create_model_name(phi)}) f({create_model_name(f)})",
                'formula': pj.create_simple_spec(phi=phi, p="~1", f=f, name=f"phi_{i}_f_{j}")
            }
            for i, phi in enumerate(phi_formulas)
            for j, f in enumerate(f_formulas)
        ]
        
        results = []
        completed_models = 0
        analysis_start_time = time.time()
    
    print(f"üìä Total models to fit: {len(model_specifications)}")
    print(f"üìä Starting from model {completed_models + 1}")
    
    # Create optimized model
    model = OptimizedPradelModel()
    
    # Fit remaining models
    for i in range(completed_models, len(model_specifications)):
        spec = model_specifications[i]
        model_start_time = time.time()
        
        elapsed_total = time.time() - analysis_start_time
        if i > completed_models:
            avg_time_per_model = elapsed_total / (i - completed_models)
            remaining_models = len(model_specifications) - i
            eta_seconds = avg_time_per_model * remaining_models
            eta_hours = eta_seconds / 3600
            
            print(f"\nüîß Model {i+1}/{len(model_specifications)}: {spec['name']}")
            print(f"   ‚è±Ô∏è  Elapsed: {elapsed_total/3600:.1f}h, ETA: {eta_hours:.1f}h")
        else:
            print(f"\nüîß Model {i+1}/{len(model_specifications)}: {spec['name']}")
        
        # Fit model
        result = fit_model_optimized(model, spec['formula'], data_context, spec['name'])
        results.append({'name': spec['name'], 'result': result})
        
        model_time = time.time() - model_start_time
        
        if result['success']:
            print(f"   ‚úÖ Success - AIC: {result['aic']:.1f}, Time: {model_time:.1f}s")
        else:
            print(f"   ‚ùå Failed: {result['error']}, Time: {model_time:.1f}s")
        
        completed_models = i + 1
        
        # Save checkpoint periodically
        if completed_models % checkpoint_interval == 0:
            checkpoint_state = {
                'results': results,
                'completed_models': completed_models,
                'data_context': data_context,
                'model_specifications': model_specifications,
                'analysis_start_time': analysis_start_time,
                'sample_size': sample_size
            }
            checkpoint_manager.save_checkpoint(checkpoint_state)
            
            # Export progress
            export_results_with_progress(
                results, 
                str(output_path / base_name), 
                completed_models, 
                len(model_specifications)
            )
    
    # Final export
    total_time = time.time() - analysis_start_time
    
    print(f"\nüéØ Analysis Complete!")
    print(f"Total time: {total_time/3600:.1f} hours")
    print(f"Average time per model: {total_time/len(model_specifications):.1f} seconds")
    
    # Final results export
    final_csv = str(output_path / f"{base_name}_final.csv")
    final_json = str(output_path / f"{base_name}_final.json")
    
    export_results_with_progress(results, str(output_path / base_name), completed_models, len(model_specifications))
    
    # Export detailed JSON
    export_data = {
        'analysis_info': {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_time_hours': total_time / 3600,
            'sample_size': sample_size,
            'total_models': len(model_specifications),
            'successful_models': sum(1 for r in results if r['result']['success'])
        },
        'models': results
    }
    
    with open(final_json, 'w') as f:
        json.dump(export_data, f, indent=2)
    
    print(f"üìä Final results: {final_csv}")
    print(f"üìä Detailed results: {final_json}")
    
    # Clean up checkpoint
    checkpoint_manager.delete_checkpoint()
    
    return results

def main():
    """Main entry point with command line arguments."""
    parser = argparse.ArgumentParser(description='Production Nebraska Analysis')
    parser.add_argument('--sample-size', type=int, default=1000, 
                       help='Number of individuals to analyze')
    parser.add_argument('--output-dir', default='results',
                       help='Output directory for results')
    parser.add_argument('--resume', action='store_true',
                       help='Resume from checkpoint if available')
    parser.add_argument('--checkpoint-interval', type=int, default=5,
                       help='Save checkpoint every N models')
    
    args = parser.parse_args()
    
    print("üî¨ Production Nebraska Analysis with Optimizations")
    print("=" * 60)
    print(f"Sample size: {args.sample_size:,} individuals")
    print(f"Output directory: {args.output_dir}")
    print(f"Resume mode: {args.resume}")
    print(f"Checkpoint interval: {args.checkpoint_interval} models")
    
    if not Path("data/encounter_histories_ne_clean.csv").exists():
        print("‚ùå Data file not found: data/encounter_histories_ne_clean.csv")
        sys.exit(1)
    
    try:
        results = run_production_analysis(
            sample_size=args.sample_size,
            output_dir=args.output_dir,
            checkpoint_interval=args.checkpoint_interval,
            resume=args.resume
        )
        
        successful = sum(1 for r in results if r['result']['success'])
        print(f"\n‚úÖ Analysis completed: {successful}/{len(results)} models successful")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Analysis interrupted by user")
        print("üíæ Progress saved in checkpoint - use --resume to continue")
    except Exception as e:
        print(f"\n‚ùå Analysis failed: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()