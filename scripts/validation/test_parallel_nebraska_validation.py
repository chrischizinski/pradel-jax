#!/usr/bin/env python3
"""
Test the fixed parallel optimization framework on real Nebraska data.

This validates that the DataContext serialization fixes work correctly
with actual production datasets and realistic model specifications.
"""

import numpy as np
import pandas as pd
import time
from pathlib import Path
import json

# Set PYTHONPATH to include our package
import sys
sys.path.insert(0, str(Path(__file__).parent))

def test_parallel_nebraska_small_sample():
    """Test parallel optimization on a small sample of Nebraska data."""
    print("Testing parallel optimization on Nebraska data sample...")
    
    # Import components
    import pradel_jax as pj
    from pradel_jax.optimization.parallel import (
        ParallelOptimizer, 
        ParallelModelSpec,
        create_model_specs_from_formulas
    )
    from pradel_jax.optimization.strategy import OptimizationStrategy
    
    # Load Nebraska data
    data_file = Path("data/encounter_histories_ne_clean.csv")
    if not data_file.exists():
        print(f"‚ùå Nebraska data file not found: {data_file}")
        print("This test requires the Nebraska dataset")
        return False
    
    try:
        # Load and sample data for manageable test
        print("Loading Nebraska data...")
        full_data = pd.read_csv(data_file, dtype={'ch': str})
        print(f"Full dataset: {len(full_data)} individuals")
        
        # Take a simple random sample to avoid complexity
        sample_size = 100
        sample_data = full_data.sample(n=min(sample_size, len(full_data)), random_state=42)
        print(f"Sample dataset: {len(sample_data)} individuals")
        
        # Save sample to temporary file and process normally
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp_file:
            sample_data.to_csv(tmp_file.name, index=False)
            tmp_file_path = tmp_file.name
        
        try:
            # Process sample data through normal pipeline
            sample_data_context = pj.load_data(tmp_file_path, dtype={'ch': str})
        finally:
            # Clean up temp file
            Path(tmp_file_path).unlink()
        
        print(f"Sample data context: {sample_data_context.n_individuals} individuals, {sample_data_context.n_occasions} occasions")
        print(f"Available covariates: {list(sample_data_context.covariates.keys())}")
        
        # Create model specifications for parallel testing
        print("\n1. Creating model specifications...")
        
        # Define formula sets based on available covariates
        phi_formulas = ["~1"]  # Start simple
        p_formulas = ["~1"]
        f_formulas = ["~1"] 
        
        # Add covariate models if available
        covariate_names = [name for name in sample_data_context.covariates.keys() 
                         if not name.endswith('_categories') and not name.endswith('_is_categorical')]
        
        if 'gender' in covariate_names:
            phi_formulas.append("~1 + gender")
            p_formulas.append("~1 + gender")
        
        if 'age' in covariate_names:
            phi_formulas.append("~1 + age")
            p_formulas.append("~1 + age")
        
        # Create model specs
        model_specs = create_model_specs_from_formulas(
            phi_formulas=phi_formulas,
            p_formulas=p_formulas,
            f_formulas=f_formulas,
            random_seed_base=42
        )
        
        # Limit to reasonable number for testing
        model_specs = model_specs[:6]  # Test first 6 models
        
        print(f"Created {len(model_specs)} model specifications:")
        for spec in model_specs:
            print(f"  - {spec.name}")
        
        # Test parallel optimization
        print("\n2. Running parallel optimization...")
        
        optimizer = ParallelOptimizer(
            n_workers=min(4, len(model_specs)),  # Use up to 4 workers
            checkpoint_dir="test_checkpoints"
        )
        
        start_time = time.time()
        results = optimizer.fit_models_parallel(
            model_specs=model_specs,
            data_context=sample_data_context,
            strategy=OptimizationStrategy.HYBRID,  # Use hybrid strategy
            batch_size=3,  # Process 3 models at a time
            checkpoint_interval=2,  # Checkpoint every 2 batches
            checkpoint_name="nebraska_test"
        )
        
        total_time = time.time() - start_time
        
        print(f"‚úÖ Parallel optimization completed in {total_time:.1f} seconds")
        
        # Analyze results
        print("\n3. Analyzing results...")
        
        successful_results = [r for r in results if r and r.success]
        failed_results = [r for r in results if r and not r.success]
        
        print(f"Successful fits: {len(successful_results)}/{len(model_specs)}")
        print(f"Failed fits: {len(failed_results)}/{len(model_specs)}")
        
        if failed_results:
            print("\nFailed models:")
            for result in failed_results:
                print(f"  - {result.model_name}: {result.error_message}")
        
        if successful_results:
            print(f"\nüìä Model Results (n={sample_data_context.n_individuals} individuals):")
            print("-" * 80)
            
            # Sort by AIC
            successful_results.sort(key=lambda r: r.aic)
            
            for i, result in enumerate(successful_results):
                rank = i + 1
                print(f"{rank:2d}. {result.model_name:25} | AIC: {result.aic:8.2f} | "
                      f"LL: {result.log_likelihood:8.2f} | Time: {result.fit_time:5.1f}s | "
                      f"Strategy: {result.strategy_used}")
            
            # Best model analysis
            best_result = successful_results[0]
            print(f"\nüèÜ Best Model: {best_result.model_name}")
            print(f"   AIC: {best_result.aic:.2f}")
            print(f"   Log-likelihood: {best_result.log_likelihood:.2f}")
            print(f"   Parameters: {len(best_result.parameters)} estimated")
            print(f"   Optimization time: {best_result.fit_time:.1f}s")
            print(f"   Strategy used: {best_result.strategy_used}")
            
            if best_result.parameters:
                params = np.array(best_result.parameters)
                print(f"   Parameter values: {params}")
                
                # Convert key parameters to probabilities for interpretation
                if len(params) >= 3:
                    phi_est = 1 / (1 + np.exp(-params[0]))  # Survival
                    p_est = 1 / (1 + np.exp(-params[1]))    # Detection  
                    f_est = 1 / (1 + np.exp(-params[2]))    # Recruitment
                    
                    print(f"   Estimated survival (œÜ): {phi_est:.3f}")
                    print(f"   Estimated detection (p): {p_est:.3f}")
                    print(f"   Estimated recruitment (f): {f_est:.3f}")
            
            # Lambda estimates if available
            if best_result.lambda_mean is not None:
                print(f"   Lambda (growth rate): {best_result.lambda_mean:.3f} ¬± {best_result.lambda_std:.3f}")
            
            # Test data reproducibility
            if best_result.data_hash:
                print(f"   Data hash: {best_result.data_hash}")
            
            return len(successful_results) > 0
        
        else:
            print("‚ùå No successful model fits")
            return False
            
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_parallel_performance_comparison():
    """Compare parallel vs sequential performance on Nebraska data."""
    print("\n" + "="*60)
    print("Testing parallel vs sequential performance...")
    
    import pradel_jax as pj
    from pradel_jax.optimization.parallel import ParallelOptimizer, ParallelModelSpec
    from pradel_jax.optimization.strategy import OptimizationStrategy
    
    try:
        # Load small sample for performance comparison
        data_file = Path("data/encounter_histories_ne_clean.csv")
        if not data_file.exists():
            print("‚ùå Nebraska data not available for performance test")
            return False
        
        # Use even smaller sample for timing comparison
        full_data = pd.read_csv(data_file, dtype={'ch': str})
        sample_data = full_data.sample(n=50, random_state=123)
        
        # Process using adapter directly for sample
        from pradel_jax.data.adapters import RMarkFormatAdapter
        adapter = RMarkFormatAdapter()
        data_context = adapter.process(sample_data)
        
        print(f"Performance test dataset: {data_context.n_individuals} individuals")
        
        # Create small set of models for timing
        model_specs = [
            ParallelModelSpec(
                name="œÜ(.) p(.) f(.)",
                formula_spec=pj.create_simple_spec(phi="~1", p="~1", f="~1"),
                index=0
            ),
            ParallelModelSpec(
                name="œÜ(gender) p(.) f(.)",  
                formula_spec=pj.create_simple_spec(phi="~1 + gender", p="~1", f="~1"),
                index=1
            ),
            ParallelModelSpec(
                name="œÜ(.) p(gender) f(.)",
                formula_spec=pj.create_simple_spec(phi="~1", p="~1 + gender", f="~1"),
                index=2
            )
        ]
        
        print(f"Testing {len(model_specs)} models")
        
        # Sequential timing (single worker)
        print("\n1. Sequential optimization (1 worker)...")
        sequential_start = time.time()
        
        optimizer_seq = ParallelOptimizer(n_workers=1)
        sequential_results = optimizer_seq.fit_models_parallel(
            model_specs=model_specs,
            data_context=data_context,
            strategy=OptimizationStrategy.SCIPY_LBFGS,
            batch_size=1
        )
        
        sequential_time = time.time() - sequential_start
        sequential_success = sum(1 for r in sequential_results if r and r.success)
        
        # Parallel timing (multiple workers)
        print("2. Parallel optimization (3 workers)...")
        parallel_start = time.time()
        
        optimizer_par = ParallelOptimizer(n_workers=3)
        parallel_results = optimizer_par.fit_models_parallel(
            model_specs=model_specs,
            data_context=data_context,
            strategy=OptimizationStrategy.SCIPY_LBFGS,
            batch_size=3  # All at once
        )
        
        parallel_time = time.time() - parallel_start
        parallel_success = sum(1 for r in parallel_results if r and r.success)
        
        # Performance comparison
        print(f"\nüìä Performance Comparison:")
        print(f"Sequential: {sequential_time:.1f}s ({sequential_success}/{len(model_specs)} successful)")
        print(f"Parallel:   {parallel_time:.1f}s ({parallel_success}/{len(model_specs)} successful)")
        
        if sequential_time > 0 and parallel_time > 0:
            speedup = sequential_time / parallel_time
            print(f"Speedup:    {speedup:.1f}x")
            
            if speedup > 1.2:  # Expect at least 20% speedup
                print("‚úÖ Parallel optimization shows performance improvement")
                return True
            else:
                print("‚ö†Ô∏è  Parallel performance similar to sequential (expected for small datasets)")
                return True  # Still successful, just no speedup
        else:
            print("‚ö†Ô∏è  Unable to measure performance accurately")
            return True
            
    except Exception as e:
        print(f"‚ùå Performance test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_parallel_robustness():
    """Test parallel optimization robustness with edge cases."""
    print("\n" + "="*60) 
    print("Testing parallel optimization robustness...")
    
    import pradel_jax as pj
    from pradel_jax.optimization.parallel import ParallelOptimizer, ParallelModelSpec
    from pradel_jax.optimization.strategy import OptimizationStrategy
    
    try:
        # Create challenging test case
        test_data = pd.DataFrame({
            'ch': ['1000', '0100', '0010', '0001', '1100', '1010', '0110', '0011'],
            'group': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],
            'value': [1.0, 2.0, 1.5, 2.5, 1.2, 2.2, 1.8, 2.8]
        })
        
        from pradel_jax.data.adapters import RMarkFormatAdapter
        adapter = RMarkFormatAdapter()
        data_context = adapter.process(test_data)
        
        print(f"Robustness test: {data_context.n_individuals} individuals")
        
        # Test edge cases
        edge_case_specs = [
            ParallelModelSpec(
                name="Simple model",
                formula_spec=pj.create_simple_spec(phi="~1", p="~1", f="~1"),
                index=0
            ),
            ParallelModelSpec(
                name="Complex model", 
                formula_spec=pj.create_simple_spec(phi="~1 + group + value", p="~1 + group", f="~1"),
                index=1
            )
        ]
        
        # Test with different strategies
        strategies = [
            OptimizationStrategy.SCIPY_LBFGS,
            OptimizationStrategy.HYBRID
        ]
        
        for strategy in strategies:
            print(f"\nTesting strategy: {strategy.value}")
            
            optimizer = ParallelOptimizer(n_workers=2)
            results = optimizer.fit_models_parallel(
                model_specs=edge_case_specs,
                data_context=data_context,
                strategy=strategy
            )
            
            successful = sum(1 for r in results if r and r.success)
            print(f"  Results: {successful}/{len(edge_case_specs)} successful")
            
            if successful > 0:
                print(f"  ‚úÖ Strategy {strategy.value} handled edge cases")
            else:
                print(f"  ‚ö†Ô∏è  Strategy {strategy.value} struggled with edge cases (not necessarily a failure)")
        
        print("‚úÖ Robustness tests completed")
        return True
        
    except Exception as e:
        print(f"‚ùå Robustness test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("Nebraska Data Parallel Optimization Test Suite")
    print("=" * 60)
    
    # Test 1: Basic Nebraska data test
    success1 = test_parallel_nebraska_small_sample()
    
    # Test 2: Performance comparison
    success2 = test_parallel_performance_comparison()
    
    # Test 3: Robustness testing
    success3 = test_parallel_robustness()
    
    # Summary
    print("\n" + "=" * 60)
    print("NEBRASKA DATA TEST SUMMARY")
    print("=" * 60)
    print(f"Nebraska data sample test: {'‚úÖ PASS' if success1 else '‚ùå FAIL'}")
    print(f"Performance comparison:    {'‚úÖ PASS' if success2 else '‚ùå FAIL'}")
    print(f"Robustness testing:        {'‚úÖ PASS' if success3 else '‚ùå FAIL'}")
    
    overall_success = success1 and success2 and success3
    
    if overall_success:
        print(f"\nüéâ ALL TESTS PASSED!")
        print("The parallel optimization framework works correctly with real Nebraska data!")
        exit(0)
    else:
        print(f"\n‚ùå SOME TESTS FAILED")
        print("Check the output above for specific issues")
        exit(1)